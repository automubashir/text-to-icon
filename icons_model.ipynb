{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/automubashir/text-to-icon/blob/main/icons_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔥 FULL AI ICON GENERATOR: Scraping → Training → Gradio UI (PNG/SVG + Icon Font Support)\n",
        "# Run in Google Colab\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import requests\n",
        "import subprocess\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from svglib.svglib import svg2rlg # Corrected import\n",
        "from reportlab.graphics import renderPM\n",
        "from pydub import AudioSegment\n",
        "import gradio as gr\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from diffusers import UNet2DConditionModel\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import shutil\n",
        "import time\n",
        "import zipfile\n",
        "import json\n",
        "import re\n",
        "\n",
        "# ================================\n",
        "# 1. CONFIGURATION\n",
        "# ================================\n",
        "\n",
        "# Set your project paths\n",
        "PROJECT_DIR = \"/content/icon-generator\"\n",
        "DATASET_DIR = os.path.join(PROJECT_DIR, \"dataset\")\n",
        "SVG_DIR = os.path.join(DATASET_DIR, \"svg\")\n",
        "PNG_DIR = os.path.join(DATASET_DIR, \"png\")\n",
        "CAPTIONS_FILE = os.path.join(DATASET_DIR, \"captions.jsonl\")\n",
        "MODEL_OUTPUT_DIR = os.path.join(PROJECT_DIR, \"trained_model\")\n",
        "GRADIO_SHARE = True  # Share UI publicly\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(SVG_DIR, exist_ok=True)\n",
        "os.makedirs(PNG_DIR, exist_ok=True)\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Model settings\n",
        "BASE_MODEL = \"runwayml/stable-diffusion-1-5\"\n",
        "RESOLUTION = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "LORA_RANK = 32\n",
        "\n",
        "# Icon font settings\n",
        "FONT_OUTPUT_DIR = os.path.join(PROJECT_DIR, \"icon_font\")\n",
        "os.makedirs(FONT_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ================================\n",
        "# 2. DOWNLOAD & SCRAPE ICONS (Material Design Icons)\n",
        "# ================================\n",
        "\n",
        "def download_material_icons():\n",
        "    print(\"🔽 Downloading Material Design Icons...\")\n",
        "    if not os.path.exists(\"/content/material-design-icons\"):\n",
        "        subprocess.run([\n",
        "            \"git\", \"clone\", \"--depth=1\",\n",
        "            \"https://github.com/google/material-design-icons.git\"\n",
        "        ], check=True)\n",
        "    print(\"✅ Downloaded!\")\n",
        "    return \"/content/material-design-icons/src\"\n",
        "\n",
        "ICON_SRC_DIR = download_material_icons()\n",
        "\n",
        "# ================================\n",
        "# 3. PROCESS SVG → PNG + CAPTIONS\n",
        "# ================================\n",
        "\n",
        "def clean_name(name):\n",
        "    return re.sub(r'[-_]+', ' ', name).title()\n",
        "\n",
        "def process_icons():\n",
        "    print(\"🔧 Processing SVGs to PNG and generating captions...\")\n",
        "    captions = []\n",
        "    count = 0\n",
        "\n",
        "    for root, _, files in os.walk(ICON_SRC_DIR):\n",
        "        for file in files:\n",
        "            if file.endswith(\".svg\"):\n",
        "                svg_path = os.path.join(root, file)\n",
        "                icon_name = clean_name(file.replace(\".svg\", \"\"))\n",
        "                category = os.path.basename(root)\n",
        "\n",
        "                # Skip if too many processed\n",
        "                if count > 500:  # limit for demo\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    # Read SVG\n",
        "                    with open(svg_path, \"r\") as f:\n",
        "                        svg_content = f.read()\n",
        "\n",
        "                    # Save SVG\n",
        "                    svg_dest = os.path.join(SVG_DIR, f\"{count:04d}.svg\")\n",
        "                    with open(svg_dest, \"w\") as f:\n",
        "                        f.write(svg_content)\n",
        "\n",
        "                    # Convert SVG to PNG\n",
        "                    # Parse SVG content into an XML element tree\n",
        "                    drawing = svg2rlg(svg_path)\n",
        "                    img = renderPM.drawToPIL(drawing)\n",
        "                    img = img.convert(\"RGBA\")\n",
        "                    img = img.resize((RESOLUTION, RESOLUTION), Image.LANCZOS)\n",
        "\n",
        "                    # Extract black/dark parts (for clean icons)\n",
        "                    r, g, b, a = img.split()\n",
        "                    bg = Image.new(\"RGBA\", img.size, (255, 255, 255))\n",
        "                    bg.paste(img, mask=a)\n",
        "                    bg = bg.convert(\"L\")  # grayscale\n",
        "                    bg = bg.point(lambda x: 0 if x < 200 else 255, mode='1')\n",
        "                    bg = bg.convert(\"L\").point(lambda x: 255 - x)  # invert\n",
        "\n",
        "                    png_img = Image.new(\"RGBA\", (RESOLUTION, RESOLUTION), (0, 0, 0, 0))\n",
        "                    color_layer = Image.new(\"RGBA\", (RESOLUTION, RESOLUTION), (0, 0, 0, 255))\n",
        "                    png_img.paste(color_layer, mask=Image.fromarray(bg))\n",
        "\n",
        "                    png_dest = os.path.join(PNG_DIR, f\"{count:04d}.png\")\n",
        "                    png_img.save(png_dest, \"PNG\")\n",
        "\n",
        "                    # Create prompt\n",
        "                    prompt = f\"{icon_name} icon, {category}, flat vector style\"\n",
        "                    captions.append({\"file\": f\"{count:04d}.png\", \"text\": prompt})\n",
        "                    count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    # Save captions\n",
        "    with open(CAPTIONS_FILE, \"w\") as f:\n",
        "        for item in captions:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "    print(f\"✅ Processed {len(captions)} icons!\")\n",
        "\n",
        "process_icons()\n",
        "\n",
        "# ================================\n",
        "# 4. TRAIN LoRA MODEL ON ICONS\n",
        "# ================================\n",
        "\n",
        "def train_lora():\n",
        "    print(\"🏋️ Starting LoRA training...\")\n",
        "\n",
        "    from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "    from diffusers.optimization import get_scheduler\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "    import accelerate\n",
        "    from tqdm import tqdm\n",
        "    import numpy as np # Moved import inside the function\n",
        "\n",
        "    class IconDataset(Dataset):\n",
        "        def __init__(self, data_file, tokenizer, img_dir):\n",
        "            self.items = []\n",
        "            with open(data_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    self.items.append(json.loads(line))\n",
        "            self.tokenizer = tokenizer\n",
        "            self.img_dir = img_dir\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.items)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = self.items[idx]\n",
        "            image = Image.open(os.path.join(self.img_dir, item['file'])).convert(\"RGB\")\n",
        "            image = image.resize((RESOLUTION, RESOLUTION))\n",
        "            image = torch.tensor(np.array(image)).permute(2, 0, 1).float() / 127.5 - 1.0\n",
        "            text = self.tokenizer(\n",
        "                item['text'],\n",
        "                max_length=77,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_ids[0]\n",
        "            return {\"input_ids\": text, \"pixel_values\": image}\n",
        "\n",
        "\n",
        "    accelerator = accelerate.Accelerator(mixed_precision=\"fp16\", gradient_accumulation_steps=1)\n",
        "    weight_dtype = torch.float16\n",
        "\n",
        "    # Load models\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(BASE_MODEL, subfolder=\"text_encoder\").to(accelerator.device, dtype=weight_dtype)\n",
        "    vae = AutoencoderKL.from_pretrained(BASE_MODEL, subfolder=\"vae\").to(accelerator.device, dtype=weight_dtype)\n",
        "    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL, subfolder=\"unet\").to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    # LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        r=LORA_RANK,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"to_q\", \"to_v\", \"to_k\", \"to_out.0\"],\n",
        "        lora_dropout=0.0,\n",
        "        bias=\"none\",\n",
        "        modules_to_save=[],\n",
        "    )\n",
        "    unet = get_peft_model(unet, lora_config)\n",
        "\n",
        "    # Dataset\n",
        "    dataset = IconDataset(CAPTIONS_FILE, tokenizer, PNG_DIR)\n",
        "    dataloader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Prepare\n",
        "    unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)\n",
        "\n",
        "    # Training loop\n",
        "    total_steps = 0\n",
        "    progress_bar = tqdm(range(len(dataloader) * NUM_EPOCHS), desc=\"Training\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        unet.train()\n",
        "        for batch in dataloader:\n",
        "            with accelerator.accumulate(unet):\n",
        "                latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample() * 0.18215\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                timesteps = torch.randint(0, 1000, (bsz,), device=latents.device)\n",
        "                noisy_latents = noise + torch.sqrt(timesteps.float().view(-1,1,1,1)/1000) * latents\n",
        "\n",
        "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "                loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                progress_bar.update(1)\n",
        "                total_steps += 1\n",
        "\n",
        "    # Save model\n",
        "    unet.save_pretrained(MODEL_OUTPUT_DIR)\n",
        "    print(f\"✅ Model saved to {MODEL_OUTPUT_DIR}\")\n",
        "\n",
        "# Uncomment to train (takes 10-20 mins on Colab)\n",
        "# train_lora()\n",
        "\n",
        "# ================================\n",
        "# 5. GRADIO UI FOR GENERATION\n",
        "# ================================\n",
        "\n",
        "def load_trained_pipeline():\n",
        "    if os.path.exists(MODEL_OUTPUT_DIR):\n",
        "        print(\"🔁 Loading fine-tuned model...\")\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            BASE_MODEL,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        pipe.unet.load_adapter(MODEL_OUTPUT_DIR)\n",
        "    else:\n",
        "        print(\"🆕 Using base model (not fine-tuned yet)...\")\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(BASE_MODEL, torch_dtype=torch.float16)\n",
        "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "    return pipe\n",
        "\n",
        "pipe = load_trained_pipeline()\n",
        "\n",
        "def generate_icon(prompt, output_format=\"png\", negative_prompt=\"text, numbers, complex background\"):\n",
        "    # Generate image\n",
        "    output = pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        width=RESOLUTION,\n",
        "        height=RESOLUTION,\n",
        "        num_inference_steps=30,\n",
        "        guidance_scale=7.0\n",
        "    )\n",
        "    image = output.images[0]\n",
        "\n",
        "    # Save as PNG\n",
        "    png_path = os.path.join(PROJECT_DIR, \"output_icon.png\")\n",
        "    image.save(png_path)\n",
        "\n",
        "    # Convert to SVG (simplified: using threshold + contour)\n",
        "    svg_path = os.path.join(PROJECT_DIR, \"output_icon.svg\")\n",
        "    img = image.convert(\"L\")\n",
        "    img = img.point(lambda x: 0 if x < 128 else 255, mode='1')\n",
        "\n",
        "    # Simple SVG (black shape)\n",
        "    w, h = img.size\n",
        "    pixels = list(img.getdata())\n",
        "    with open(svg_path, \"w\") as f:\n",
        "        f.write(f'<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"{w}\" height=\"{h}\" viewBox=\"0 0 {w} {h}\">\\n')\n",
        "        f.write('<path d=\"')\n",
        "        for y in range(h):\n",
        "            for x in range(w):\n",
        "                if pixels[y * w + x] == 0:\n",
        "                    f.write(f\"M{x},{y}h1v1h-1z\")\n",
        "        f.write('\" fill=\"black\"/>\\n</svg>')\n",
        "\n",
        "    if output_format == \"png\":\n",
        "        return png_path\n",
        "    elif output_format == \"svg\":\n",
        "        return svg_path\n",
        "\n",
        "# Launch Gradio\n",
        "demo = gr.Interface(\n",
        "    fn=generate_icon,\n",
        "    inputs=[\n",
        "        gr.Textbox(value=\"home icon, flat design\", label=\"Prompt\"),\n",
        "        gr.Radio([\"png\", \"svg\"], value=\"png\", label=\"Output Format\"),\n",
        "    ],\n",
        "    outputs=gr.Image(type=\"filepath\", label=\"Generated Icon\"),\n",
        "    title=\"🎨 AI Icon & Icon Font Generator\",\n",
        "    description=\"Generate icons from text. Outputs PNG or simplified SVG.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 6. ICON FONT GENERATION (via SVG → Font)\n",
        "# ================================\n",
        "\n",
        "def create_icon_font():\n",
        "    try:\n",
        "        from fontTools.ttLib import TTFont\n",
        "        import fontforge\n",
        "    except:\n",
        "        !apt-get update && apt-get install -y fontforge python3-fontforge\n",
        "        import fontforge\n",
        "\n",
        "    font = fontforge.font()\n",
        "    font.fontname = \"AIIconFont\"\n",
        "    font.fullname = \"AI Generated Icon Font\"\n",
        "    font.familyname = \"AIIconFont\"\n",
        "\n",
        "    codepoint = 0xE001\n",
        "    for svg_file in os.listdir(SVG_DIR)[:100]:  # limit to 100\n",
        "        file_path = os.path.join(SVG_DIR, svg_file)\n",
        "        glyph = font.createChar(codepoint)\n",
        "        glyph.importOutlines(file_path)\n",
        "        glyph.left_side_bearing = 50\n",
        "        glyph.right_side_bearing = 50\n",
        "        codepoint += 1\n",
        "\n",
        "    font_path = os.path.join(FONT_OUTPUT_DIR, \"AIIconFont.ttf\")\n",
        "    font.generate(font_path)\n",
        "    print(f\"✅ Icon font saved to {font_path}\")\n",
        "    return font_path\n",
        "\n",
        "print(\"✅ Setup complete. Use the button below to generate an icon font.\")\n",
        "gr.Interface(lambda: create_icon_font(), inputs=None, outputs=\"file\", title=\"Generate Icon Font\").launch(share=False)\n",
        "\n",
        "# ================================\n",
        "# 7. LAUNCH GRADIO UI\n",
        "# ================================\n",
        "\n",
        "print(\"🎉 Starting Gradio UI...\")\n",
        "demo.launch(share=GRADIO_SHARE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoXAxW6rcTV1",
        "outputId": "ba1c7e99-368c-44b7-cb04-7b3d04696b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔽 Downloading Material Design Icons...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}