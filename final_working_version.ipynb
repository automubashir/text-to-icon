{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/automubashir/text-to-icon/blob/main/final_working_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y python3-cairo libpango1.0-0 libpangocairo-1.0-0\n",
        "!pip install pycairo\n",
        "!pip install reportlab==3.6.12 svglib==1.5.1"
      ],
      "metadata": {
        "id": "lf5bT3L-yPC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üî• FULL AI ICON GENERATOR: Scraping ‚Üí Training ‚Üí Gradio UI (PNG/SVG + Icon Font Support)\n",
        "# Run in Google Colab\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import requests\n",
        "import subprocess\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from svglib.svglib import svg2rlg\n",
        "from reportlab.graphics import renderPM\n",
        "from pydub import AudioSegment\n",
        "import gradio as gr\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from diffusers import UNet2DConditionModel\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from peft import PeftModel, LoraConfig, get_peft_model\n",
        "import shutil\n",
        "import time\n",
        "import zipfile\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 1. CONFIGURATION\n",
        "# ================================\n",
        "\n",
        "# Set your project paths\n",
        "PROJECT_DIR = \"/content/icon-generator\"\n",
        "DATASET_DIR = os.path.join(PROJECT_DIR, \"dataset\")\n",
        "SVG_DIR = os.path.join(DATASET_DIR, \"svg\")\n",
        "PNG_DIR = os.path.join(DATASET_DIR, \"png\")\n",
        "CAPTIONS_FILE = os.path.join(DATASET_DIR, \"captions.jsonl\")\n",
        "MODEL_OUTPUT_DIR = os.path.join(PROJECT_DIR, \"trained_model\")\n",
        "GRADIO_SHARE = True  # Share UI publicly\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(SVG_DIR, exist_ok=True)\n",
        "os.makedirs(PNG_DIR, exist_ok=True)\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Model settings\n",
        "BASE_MODEL = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
        "RESOLUTION = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "LORA_RANK = 32\n",
        "\n",
        "# Icon font settings\n",
        "FONT_OUTPUT_DIR = os.path.join(PROJECT_DIR, \"icon_font\")\n",
        "os.makedirs(FONT_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ================================\n",
        "# 2. DOWNLOAD & SCRAPE ICONS (Material Design Icons)\n",
        "# ================================\n",
        "\n",
        "def download_material_icons():\n",
        "    print(\"üîΩ Downloading Material Design Icons...\")\n",
        "    if not os.path.exists(\"/content/material-design-icons\"):\n",
        "        subprocess.run([\n",
        "            \"git\", \"clone\", \"--depth=1\",\n",
        "            \"https://github.com/google/material-design-icons.git\"\n",
        "        ], check=True)\n",
        "    print(\"‚úÖ Downloaded!\")\n",
        "    return \"/content/material-design-icons/src\"\n",
        "\n",
        "ICON_SRC_DIR = download_material_icons()\n",
        "\n",
        "# ================================\n",
        "# 3. PROCESS SVG ‚Üí PNG + CAPTIONS\n",
        "# ================================\n",
        "\n",
        "def clean_name(name):\n",
        "    return re.sub(r'[-_]+', ' ', name).title()\n",
        "\n",
        "def process_icons():\n",
        "    print(\"üîß Processing SVGs to PNG and generating captions...\")\n",
        "    captions = []\n",
        "    count = 0\n",
        "\n",
        "    for root, _, files in os.walk(ICON_SRC_DIR):\n",
        "        for file in files:\n",
        "            if file.endswith(\".svg\"):\n",
        "                svg_path = os.path.join(root, file)\n",
        "                icon_name = clean_name(file.replace(\".svg\", \"\"))\n",
        "                category = os.path.basename(root)\n",
        "\n",
        "                # Skip if too many processed\n",
        "                if count > 500:  # limit for demo\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    # Read SVG\n",
        "                    with open(svg_path, \"r\") as f:\n",
        "                        svg_content = f.read()\n",
        "\n",
        "                    # Save SVG\n",
        "                    svg_dest = os.path.join(SVG_DIR, f\"{count:04d}.svg\")\n",
        "                    with open(svg_dest, \"w\") as f:\n",
        "                        f.write(svg_content)\n",
        "\n",
        "                    # Convert SVG to PNG\n",
        "                    # Parse SVG content into an XML element tree\n",
        "                    # Step 1: Convert to BytesIO from **encoded string**\n",
        "                    svg_bytes = BytesIO(svg_content.encode(\"utf-8\"))\n",
        "\n",
        "                    # Step 2: Make sure to reset the stream position\n",
        "                    svg_bytes.seek(0)\n",
        "\n",
        "\n",
        "                    # Convert SVG to ReportLab Drawing\n",
        "                    drawing = svg2rlg(svg_bytes)\n",
        "\n",
        "                    if drawing is None:\n",
        "                        raise ValueError(\"Failed to parse SVG.\")\n",
        "                    img = renderPM.drawToPIL(drawing)\n",
        "                    img = img.convert(\"RGBA\")\n",
        "                    img = img.resize((RESOLUTION, RESOLUTION), Image.LANCZOS)\n",
        "\n",
        "                    # Extract black/dark parts (for clean icons)\n",
        "                    r, g, b, a = img.split()\n",
        "\n",
        "                    # Composite onto white background using alpha channel\n",
        "                    bg = Image.new(\"RGBA\", img.size, (255, 255, 255))\n",
        "                    bg.paste(img, mask=a)\n",
        "\n",
        "                    # Convert to grayscale\n",
        "                    bg = bg.convert(\"L\")\n",
        "\n",
        "                    # Threshold to get binary mask\n",
        "                    bg = bg.point(lambda x: 0 if x < 200 else 255, mode='1')\n",
        "\n",
        "                    # Invert back to get mask for icon\n",
        "                    bg = bg.convert(\"L\").point(lambda x: 255 - x)\n",
        "\n",
        "                    # Create transparent background and black color layer\n",
        "                    png_img = Image.new(\"RGBA\", (RESOLUTION, RESOLUTION), (0, 0, 0, 0))\n",
        "                    color_layer = Image.new(\"RGBA\", (RESOLUTION, RESOLUTION), (0, 0, 0, 255))\n",
        "\n",
        "                    # ‚úÖ Use bg directly as mask (it's already a valid 'L' image)\n",
        "                    png_img.paste(color_layer, mask=bg)\n",
        "\n",
        "                    png_dest = os.path.join(PNG_DIR, f\"{int(count):04d}.png\")\n",
        "                    png_img.save(png_dest, \"PNG\")\n",
        "\n",
        "                    # Create prompt\n",
        "                    prompt = f\"{icon_name} icon, {category}, flat vector style\"\n",
        "                    captions.append({\"file\": f\"{count:04d}.png\", \"text\": prompt})\n",
        "                    count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    # Save captions\n",
        "    with open(CAPTIONS_FILE, \"w\") as f:\n",
        "        for item in captions:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Processed {len(captions)} icons!\")\n",
        "\n",
        "process_icons()\n",
        "\n",
        "# ================================\n",
        "# 4. TRAIN LoRA MODEL ON ICONS\n",
        "# ================================\n",
        "\n",
        "def train_lora():\n",
        "    print(\"üèãÔ∏è Starting LoRA training...\")\n",
        "\n",
        "    from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "    from diffusers.optimization import get_scheduler\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "    import accelerate\n",
        "    from tqdm import tqdm\n",
        "    import numpy as np # Moved import inside the function\n",
        "\n",
        "    class IconDataset(Dataset):\n",
        "        def __init__(self, data_file, tokenizer, img_dir):\n",
        "            self.items = []\n",
        "            with open(data_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    self.items.append(json.loads(line))\n",
        "            self.tokenizer = tokenizer\n",
        "            self.img_dir = img_dir\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.items)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = self.items[idx]\n",
        "            image = Image.open(os.path.join(self.img_dir, item['file'])).convert(\"RGB\")\n",
        "            image = image.resize((RESOLUTION, RESOLUTION))\n",
        "            image = torch.tensor(np.array(image)).permute(2, 0, 1).float() / 127.5 - 1.0\n",
        "            text = self.tokenizer(\n",
        "                item['text'],\n",
        "                max_length=77,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_ids[0]\n",
        "            return {\"input_ids\": text, \"pixel_values\": image}\n",
        "\n",
        "\n",
        "    accelerator = accelerate.Accelerator(mixed_precision=\"fp16\", gradient_accumulation_steps=1)\n",
        "    weight_dtype = torch.float16\n",
        "\n",
        "    # Load models\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(BASE_MODEL, subfolder=\"text_encoder\").to(accelerator.device, dtype=weight_dtype)\n",
        "    vae = AutoencoderKL.from_pretrained(BASE_MODEL, subfolder=\"vae\").to(accelerator.device, dtype=weight_dtype)\n",
        "    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL, subfolder=\"unet\").to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    # LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        r=LORA_RANK,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"to_q\", \"to_v\", \"to_k\", \"to_out.0\"],\n",
        "        lora_dropout=0.0,\n",
        "        bias=\"none\",\n",
        "        modules_to_save=[],\n",
        "    )\n",
        "    unet = get_peft_model(unet, lora_config)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    dataset = IconDataset(CAPTIONS_FILE, tokenizer, PNG_DIR)\n",
        "    # Only prepare if dataset is valid\n",
        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "    # Prepare\n",
        "    unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)\n",
        "\n",
        "    # Training loop\n",
        "    total_steps = 0\n",
        "    progress_bar = tqdm(range(len(dataloader) * NUM_EPOCHS), desc=\"Training\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        unet.train()\n",
        "        for batch in dataloader:\n",
        "            with accelerator.accumulate(unet):\n",
        "                latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample() * 0.18215\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                timesteps = torch.randint(0, 1000, (bsz,), device=latents.device)\n",
        "                noisy_latents = noise + torch.sqrt(timesteps.float().view(-1,1,1,1)/1000) * latents\n",
        "\n",
        "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "                loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                progress_bar.update(1)\n",
        "                total_steps += 1\n",
        "\n",
        "    # Save model\n",
        "    torch.save(unet.state_dict(), os.path.join(MODEL_OUTPUT_DIR, \"pytorch_model.bin\"))\n",
        "    print(f\"‚úÖ Model saved to {MODEL_OUTPUT_DIR}\")\n",
        "\n",
        "# Uncomment to train (takes 10-20 mins on Colab)\n",
        "train_lora()\n",
        "\n",
        "# ================================\n",
        "# 5. GRADIO UI FOR GENERATION\n",
        "# ================================\n",
        "\n",
        "def load_trained_pipeline():\n",
        "    if os.path.exists(MODEL_OUTPUT_DIR):\n",
        "        print(\"üîÅ Loading fine-tuned model with LoRA adapter...\")\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            BASE_MODEL,\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "\n",
        "        # Rebuild the LoRA-wrapped UNet\n",
        "        lora_config = LoraConfig(\n",
        "            r=LORA_RANK,\n",
        "            lora_alpha=16,\n",
        "            target_modules=[\"to_q\", \"to_v\", \"to_k\", \"to_out.0\"],\n",
        "            lora_dropout=0.0,\n",
        "            bias=\"none\",\n",
        "            modules_to_save=[],\n",
        "        )\n",
        "        pipe.unet = get_peft_model(pipe.unet, lora_config)\n",
        "\n",
        "        # Load LoRA adapter weights\n",
        "        pipe.unet.load_state_dict(torch.load(os.path.join(MODEL_OUTPUT_DIR, \"pytorch_model.bin\")))\n",
        "\n",
        "    else:\n",
        "        print(\"üÜï Using base model (not fine-tuned yet)...\")\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            BASE_MODEL,\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "\n",
        "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "    return pipe\n",
        "\n",
        "pipe = load_trained_pipeline()\n",
        "\n",
        "def generate_icon(prompt, output_format=\"png\", negative_prompt=\"text, numbers, complex background\"):\n",
        "    # Generate image\n",
        "    output = pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        width=RESOLUTION,\n",
        "        height=RESOLUTION,\n",
        "        num_inference_steps=30,\n",
        "        guidance_scale=7.0\n",
        "    )\n",
        "    image = output.images[0]\n",
        "\n",
        "    # Save as PNG\n",
        "    png_path = os.path.join(PROJECT_DIR, \"output_icon.png\")\n",
        "    image.save(png_path)\n",
        "\n",
        "    # Convert to SVG (simplified: using threshold + contour)\n",
        "    svg_path = os.path.join(PROJECT_DIR, \"output_icon.svg\")\n",
        "    img = image.convert(\"L\")\n",
        "    img = img.point(lambda x: 0 if x < 128 else 255, mode='1')\n",
        "\n",
        "    # Simple SVG (black shape)\n",
        "    w, h = img.size\n",
        "    pixels = list(img.getdata())\n",
        "    with open(svg_path, \"w\") as f:\n",
        "        f.write(f'<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"{w}\" height=\"{h}\" viewBox=\"0 0 {w} {h}\">\\n')\n",
        "        f.write('<path d=\"')\n",
        "        for y in range(h):\n",
        "            for x in range(w):\n",
        "                if pixels[y * w + x] == 0:\n",
        "                    f.write(f\"M{x},{y}h1v1h-1z\")\n",
        "        f.write('\" fill=\"black\"/>\\n</svg>')\n",
        "\n",
        "    if output_format == \"png\":\n",
        "        return png_path\n",
        "    elif output_format == \"svg\":\n",
        "        return svg_path\n",
        "\n",
        "# Launch Gradio\n",
        "demo = gr.Interface(\n",
        "    fn=generate_icon,\n",
        "    inputs=[\n",
        "        gr.Textbox(value=\"home icon, flat design\", label=\"Prompt\"),\n",
        "        gr.Radio([\"png\", \"svg\"], value=\"png\", label=\"Output Format\"),\n",
        "    ],\n",
        "    outputs=gr.Image(type=\"filepath\", label=\"Generated Icon\"),\n",
        "    title=\"üé® AI Icon & Icon Font Generator\",\n",
        "    description=\"Generate icons from text. Outputs PNG or simplified SVG.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 6. ICON FONT GENERATION (via SVG ‚Üí Font)\n",
        "# ================================\n",
        "\n",
        "def create_icon_font():\n",
        "    try:\n",
        "        from fontTools.ttLib import TTFont\n",
        "        import fontforge\n",
        "    except:\n",
        "        !apt-get update && apt-get install -y fontforge python3-fontforge\n",
        "        import fontforge\n",
        "\n",
        "    font = fontforge.font()\n",
        "    font.fontname = \"AIIconFont\"\n",
        "    font.fullname = \"AI Generated Icon Font\"\n",
        "    font.familyname = \"AIIconFont\"\n",
        "\n",
        "    codepoint = 0xE001\n",
        "    for svg_file in os.listdir(SVG_DIR)[:100]:  # limit to 100\n",
        "        file_path = os.path.join(SVG_DIR, svg_file)\n",
        "        glyph = font.createChar(codepoint)\n",
        "        glyph.importOutlines(file_path)\n",
        "        glyph.left_side_bearing = 50\n",
        "        glyph.right_side_bearing = 50\n",
        "        codepoint += 1\n",
        "\n",
        "    font_path = os.path.join(FONT_OUTPUT_DIR, \"AIIconFont.ttf\")\n",
        "    font.generate(font_path)\n",
        "    print(f\"‚úÖ Icon font saved to {font_path}\")\n",
        "    return font_path\n",
        "\n",
        "print(\"‚úÖ Setup complete. Use the button below to generate an icon font.\")\n",
        "gr.Interface(lambda: create_icon_font(), inputs=None, outputs=\"file\", title=\"Generate Icon Font\").launch(share=False)\n",
        "\n",
        "# ================================\n",
        "# 7. LAUNCH GRADIO UI\n",
        "# ================================\n",
        "\n",
        "print(\"üéâ Starting Gradio UI...\")\n",
        "demo.launch(share=GRADIO_SHARE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7XWlg9koklk",
        "outputId": "b6af8414-9c6d-45e3-bdd7-f7ad6de50fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîΩ Downloading Material Design Icons...\n",
            "‚úÖ Downloaded!\n",
            "üîß Processing SVGs to PNG and generating captions...\n",
            "‚úÖ Processed 501 icons!\n",
            "üèãÔ∏è Starting LoRA training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining:   0%|          | 0/1260 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1GAWXW80mrW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}